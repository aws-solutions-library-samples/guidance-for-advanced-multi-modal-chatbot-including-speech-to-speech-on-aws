AWSTemplateFormatVersion: '2010-09-09'
Description: Chatbot to talk with your documents, images video and audio. Currently supports us-west-2 only (Bedrock Data Automation preview is currently available only in us-west-2). Deploy lambda-edge.yaml after this stack is created.

Parameters:
  ModelId:
    Type: String
    Default: "us.anthropic.claude-3-haiku-20240307-v1:0"
    Description: The Amazon Bedrock supported LLM inference profile ID used for inference.
  EmbeddingModelId:
    Type: String
    Default: "amazon.titan-embed-text-v2:0"
    Description: The Amazon Bedrock supported embedding LLM ID used in Bedrock Knowledgebase.
  DataParser:
    Type: String
    Default: "Bedrock Data Automation"
    AllowedValues:
      - "Bedrock Data Automation"
    Description: |
      Bedrock Data Automation: Bedrock Data Automation processes visually rich documents, images, videos and audio and converts to text.
  ResourceSuffix:
    Type: String
    Description: Suffix to append to resource names (e.g., dev, test, prod)
    AllowedPattern: ^[a-zA-Z0-9-]*$
    ConstraintDescription: Only alphanumeric characters and hyphens are allowed
    MinLength: '1'
    MaxLength: '20'

Conditions:
  IsBedrockDefault: !Equals 
    - !Ref DataParser
    - "Transcribe + Bedrock Default"
  IsTranscribeWithBDA: !Equals 
    - !Ref DataParser
    - "Transcript + Bedrock Data Automation"
  IsBedrockDataAutomation: !Equals 
    - !Ref DataParser
    - "Bedrock Data Automation"
  IsBedrockDefaultOrBDA: !Or
    - Condition: IsBedrockDefault
    - Condition: IsBedrockDataAutomation
  IsTranscribe: !Or
    - Condition: IsBedrockDefault
    - Condition: IsTranscribeWithBDA

Resources:
  MediaBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub ${AWS::AccountId}-media-bucket-${AWS::StackName}-${ResourceSuffix}
      NotificationConfiguration:
        EventBridgeConfiguration:
          EventBridgeEnabled: true
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      CorsConfiguration:
        CorsRules:
          - AllowedHeaders:
              - "*"
            AllowedMethods:
              - GET
              - PUT
              - POST
              - DELETE
            AllowedOrigins:
              - "*"
            ExposedHeaders:
              - "ETag"

  OrganizedBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub ${AWS::AccountId}-organized-bucket-${AWS::StackName}-${ResourceSuffix}
      NotificationConfiguration:
        EventBridgeConfiguration:
          EventBridgeEnabled: true

  MultimodalBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub ${AWS::AccountId}-multimodal-bucket-${AWS::StackName}-${ResourceSuffix}
      NotificationConfiguration:
        EventBridgeConfiguration:
          EventBridgeEnabled: true

  LayerBucket:
    Type: AWS::S3::Bucket
    Properties:
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  LayerCreatorRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: S3Access
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                Resource: !Sub '${LayerBucket.Arn}/*'

  LayerCreatorFunction:
      Type: AWS::Lambda::Function
      Properties:
        Runtime: python3.12
        Handler: index.handler
        Role: !GetAtt LayerCreatorRole.Arn
        MemorySize: 512
        Timeout: 900
        Code:
          ZipFile: |
            import boto3
            import cfnresponse
            import os
            import subprocess
            import shutil
            import zipfile
            
            def handler(event, context):
                try:
                    if event['RequestType'] in ['Create', 'Update']:
                        # Get properties
                        bucket = event['ResourceProperties']['Bucket']
                        key = 'layer.zip'
                        
                        # Create working directory
                        os.makedirs('/tmp/python/lib/python3.12/site-packages', exist_ok=True)
                        
                        # Install packages with --upgrade to ensure latest versions
                        subprocess.check_call([
                            'pip', 'install',
                            '--platform', 'manylinux2014_x86_64',
                            '--implementation', 'cp',
                            '--only-binary=:all:',
                            '--upgrade',  # This flag ensures we get the latest versions
                            '--target', '/tmp/python/lib/python3.12/site-packages',
                            'boto3',
                            'botocore',
                            'opensearch-py',
                            'requests-aws4auth'
                        ])
                        
                        # Create ZIP file
                        shutil.make_archive('/tmp/layer', 'zip', '/tmp')
                        
                        # Upload to S3
                        s3 = boto3.client('s3')
                        s3.upload_file('/tmp/layer.zip', bucket, key)
                        
                        cfnresponse.send(event, context, cfnresponse.SUCCESS, {
                            'Bucket': bucket,
                            'Key': key
                        })
                    else:
                        cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                except Exception as e:
                    print(f"Error: {str(e)}")
                    cfnresponse.send(event, context, cfnresponse.FAILED, {})
  
  CreateLayer:
    Type: Custom::LayerCreator
    Properties:
      ServiceToken: !GetAtt LayerCreatorFunction.Arn
      Bucket: !Ref LayerBucket

  DependencyLayer:
      Type: AWS::Lambda::LayerVersion
      DependsOn: CreateLayer
      Properties:
        CompatibleRuntimes:
          - python3.12
        Content:
          S3Bucket: !Ref LayerBucket
          S3Key: layer.zip
        Description: Layer for dependencies

  # Amazon Bedrock Data Automation Resources
  BDAProjectCreatorFunction:
    Type: AWS::Lambda::Function
    Condition: IsBedrockDataAutomation
    Properties:
      FunctionName: !Sub '${AWS::StackName}-bda-project-creator-${ResourceSuffix}'
      Runtime: python3.12
      Handler: index.handler
      Role: !GetAtt BDAProjectCreatorRole.Arn
      Layers:
        - !Ref DependencyLayer
      MemorySize: 256
      Timeout: 300
      Environment:
        Variables:
          STACK_NAME: !Ref 'AWS::StackName'
          RESOURCE_SUFFIX: !Ref ResourceSuffix
          REGION: !Ref 'AWS::Region'
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import os
          from botocore.exceptions import ClientError
  
          def handler(event, context):
              try:
                  if event['RequestType'] in ['Create', 'Update']:
                      # Get stack name and resource suffix from environment variables
                      stack_name = os.environ.get('STACK_NAME')
                      resource_suffix = os.environ.get('RESOURCE_SUFFIX')
                      region = os.environ.get('REGION')
                      project_name = f"{stack_name}-bda-project-{resource_suffix}"
                      
                      # Create Bedrock Data Automation client
                      bda_client = boto3.client('bedrock-data-automation', region)
                      
                      # Define standard output configuration
                      standard_output_config = {
                          "document": {
                              "extraction": {
                                  "granularity": {
                                      "types": [
                                          "PAGE",
                                          "ELEMENT",
                                          "WORD"
                                      ]
                                  },
                                  "boundingBox": {
                                      "state": "ENABLED"
                                  }
                              },
                              "generativeField": {
                                  "state": "ENABLED"
                              },
                              "outputFormat": {
                                  "textFormat": {
                                      "types": [
                                          "PLAIN_TEXT"
                                      ]
                                  },
                                  "additionalFileFormat": {
                                      "state": "ENABLED"
                                  }
                              }
                          },
                          "image": {
                              "extraction": {
                                  "category": {
                                      "state": "ENABLED",
                                      "types": [
                                          "TEXT_DETECTION",
                                          "CONTENT_MODERATION"
                                      ]
                                  },
                                  "boundingBox": {
                                      "state": "ENABLED"
                                  }
                              },
                              "generativeField": {
                                  "state": "ENABLED",
                                  "types": [
                                      "IMAGE_SUMMARY",
                                      "IAB"
                                  ]
                              }
                          },
                          "video": {
                              "extraction": {
                                  "category": {
                                      "state": "ENABLED",
                                      "types": [
                                          "TRANSCRIPT",
                                          "TEXT_DETECTION",
                                          "CONTENT_MODERATION"
                                      ]
                                  },
                                  "boundingBox": {
                                      "state": "ENABLED"
                                  }
                              },
                              "generativeField": {
                                  "state": "ENABLED",
                                  "types": [
                                      "VIDEO_SUMMARY",
                                      "CHAPTER_SUMMARY",
                                      "IAB"
                                  ]
                              }
                          },
                          "audio": {
                              "extraction": {
                                  "category": {
                                      "state": "ENABLED",
                                      "types": [
                                          "TRANSCRIPT",
                                          "AUDIO_CONTENT_MODERATION"
                                      ]
                                  }
                              },
                              "generativeField": {
                                  "state": "ENABLED",
                                  "types": [
                                      "AUDIO_SUMMARY",
                                      "TOPIC_SUMMARY"
                                  ]
                              }
                          }
                      }
                      
                      try:
                          # Create the project
                          response = bda_client.create_data_automation_project(
                              projectName=project_name,
                              projectDescription=f"Data automation project for {project_name}",
                              projectStage='LIVE',
                              standardOutputConfiguration=standard_output_config,
                              overrideConfiguration={
                                  'document': {
                                      'splitter': {
                                          'state': 'ENABLED'
                                      }
                                  }
                              }
                          )
                          
                          print(f"Project created successfully with ARN: {response['projectArn']}")
                          cfnresponse.send(event, context, cfnresponse.SUCCESS, {
                              'ProjectArn': response['projectArn'],
                              'ProjectName': project_name
                          })
                      except ClientError as e:
                          print(f"Error: {str(e)}")
                          cfnresponse.send(event, context, cfnresponse.FAILED, {
                              'Error': str(e)
                          })
                  else:
                      # Handle DELETE request
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
              except Exception as e:
                  print(f"Error: {str(e)}")
                  cfnresponse.send(event, context, cfnresponse.FAILED, {})
  
  BDAProjectCreatorRole:
    Type: AWS::IAM::Role
    Condition: IsBedrockDataAutomation
    Properties:
      RoleName: !Sub ${AWS::AccountId}-bda-project-creator-role-${AWS::StackName}-${ResourceSuffix}
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: BDAAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'bedrock:CreateDataAutomationProject'
                Resource: !Sub 'arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:data-automation-project/*'
  
  BDAProjectCreator:
    Type: Custom::BDAProjectCreator
    Condition: IsBedrockDataAutomation
    Properties:
      Name: !Sub '${AWS::StackName}-bda-project-${ResourceSuffix}'
      ServiceToken: !GetAtt BDAProjectCreatorFunction.Arn
  
  BDAEventRule:
    Type: AWS::Events::Rule
    Condition: IsBedrockDataAutomation
    Properties:
      Name: !Sub '${AWS::StackName}-bda-async-rule-${ResourceSuffix}'
      Description: 'Rule for BDA async API calls'
      EventPattern:
        source:
          - aws.bedrock
          - aws.bedrock-test
        detail-type:
          - "Bedrock Data Automation Job Succeeded"
          - "Bedrock Data Automation Job Failed With Client Error"
          - "Bedrock Data Automation Job Failed With Service Error"
      State: ENABLED
      Targets:
        - Arn: !GetAtt BDAProcessingFunction.Arn
          Id: "BDAProcessingTarget"

  BDAProcessingFunction:
    Type: AWS::Lambda::Function
    Condition: IsBedrockDataAutomation
    Properties:
      FunctionName: !Sub '${AWS::StackName}-bda-processor-${ResourceSuffix}'
      Layers:
        - !Ref DependencyLayer
      Handler: index.lambda_handler
      Role: !GetAtt BDAProcessingFunctionRole.Arn
      Code:
        ZipFile: |
          from typing import Dict, List, Any
          import json, os, logging, boto3
          from operator import itemgetter
          from functools import reduce

          # Set up logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          class ContentProcessor:
              def get_time_range(self, timestamps: List[int]) -> str:
                  """
                  Get the time range from a list of timestamps.
                  
                  Args:
                      timestamps (List[int]): List of timestamp values
                      
                  Returns:
                      str: Time range in format "min-max" or "0-0" if empty
                  """
                  try:
                      logger.info(f"Processing {len(timestamps)} timestamps")
                      
                      if not timestamps:
                          logger.info("Empty timestamp list received, returning default range")
                          return "0-0"
                      
                      # Single pass through the list instead of two separate passes for min/max
                      if len(timestamps) == 1:
                          return f"{timestamps[0]}-{timestamps[0]}"
                          
                      min_max = reduce(
                          lambda acc, x: (min(acc[0], x), max(acc[1], x)),
                          timestamps[1:],
                          (timestamps[0], timestamps[0])
                      )
                      
                      result = f"{min_max[0]}-{min_max[1]}"
                      logger.info(f"Time range calculated: {result}")
                      return result
                      
                  except Exception as e:
                      logger.error(f"Error calculating time range: {str(e)}", exc_info=True)
                      raise

              def format_summary_section(self, summary: str) -> str:
                  """
                  Formats the summary section with proper markdown heading.
                  
                  Args:
                      summary (str): Raw summary text
                  Returns:
                      str: Formatted summary with markdown heading
                  """
                  try:
                      logger.debug(f"Formatting summary section, length: {len(summary)}")
                      
                      # Pre-validate input
                      if not summary:
                          logger.warning("Empty summary received")
                          return "# Summary\n"
                      
                      # Use stripped version only once and store
                      formatted_summary = f"# Summary\n{summary.strip()}"
                      logger.debug(f"Summary section formatted successfully")
                      
                      return formatted_summary
                      
                  except Exception as e:
                      logger.error(f"Error formatting summary section: {str(e)}", exc_info=True)
                      raise

              def group_content_by_timerange(
                      self, 
                      content_list: List[Dict[str, Any]]
                  ) -> List[List[Dict[str, Any]]]:
                      """
                      Groups content items by time range chunks.
                      
                      Args:
                          content_list: List of dictionaries containing timestamp data
                      Returns:
                          List of grouped content items
                      """
                      try:
                          # Get chunk_size from environment variable with fallback
                          chunk_size = int(os.getenv('CHUNK_SIZE_MS', '60000'))
                          
                          if not content_list:
                              logger.info("Empty content list received")
                              return []
                  
                          logger.info(f"Grouping {len(content_list)} items with chunk size {chunk_size}ms")
                          
                          # Pre-sort using operator.itemgetter for better performance
                          from operator import itemgetter
                          sorted_content = sorted(content_list, key=itemgetter('timestamp'))
                          
                          grouped_content = []
                          current_group = []
                          start_time = sorted_content[0]['timestamp']
                          
                          # Use enumerate for better performance monitoring
                          for idx, item in enumerate(sorted_content):
                              current_time = item['timestamp']
                              time_diff = current_time - start_time
                              
                              if not current_group or time_diff <= chunk_size:
                                  current_group.append(item)
                              else:
                                  grouped_content.append(current_group)
                                  current_group = [item]
                                  start_time = current_time
                                  
                              # Log progress for large datasets
                              if (idx + 1) % 10000 == 0:
                                  logger.debug(f"Processed {idx + 1} items, created {len(grouped_content)} groups")
                          
                          # Add the last group if it exists
                          if current_group:
                              grouped_content.append(current_group)
                          
                          logger.info(f"Grouping completed: created {len(grouped_content)} groups")
                          return grouped_content
                          
                      except ValueError as ve:
                          logger.error(f"Invalid CHUNK_SIZE_MS environment variable: {str(ve)}", exc_info=True)
                          raise
                      except Exception as e:
                          logger.error(f"Error grouping content: {str(e)}", exc_info=True)
                          raise
              
              def _binary_search(self, arr: List[Dict[str, Any]], target: int, start: int = 0) -> int:
                  """Helper method for binary search on timestamp arrays."""
                  left, right = start, len(arr) - 1
                  
                  while left <= right:
                      mid = (left + right) // 2
                      if arr[mid]['timestamp'] == target:
                          return mid
                      elif arr[mid]['timestamp'] < target:
                          left = mid + 1
                      else:
                          right = mid - 1
                          
                  return left
              
              def format_content_group(self, group: List[Dict[str, Any]], _: List[Dict[str, Any]]) -> str:
                  """
                  Formats content group with all content in chronological order.
                  """
                  try:
                      if not group:
                          logger.debug("Empty group received")
                          return ""

                      timestamps = [item['timestamp'] for item in group]
                      start_time, end_time = min(timestamps), max(timestamps)
                      
                      logger.debug(f"Processing content group: {start_time} to {end_time}")

                      output_lines = []
                      output_lines.append(f"\n[{start_time}] to [{end_time}]")

                      # Sort content by timestamp
                      sorted_content = sorted(group, key=itemgetter('timestamp'))
                      
                      for item in sorted_content:
                          # Check if this is an audio transcript (contains speaker name)
                          if 'type' in item and item['type'] == 'Audio':
                              output_lines.append(
                                  f"[{item['timestamp']}] Audio Content: {item['text']}"
                              )
                          else:
                              output_lines.append(
                                  f"[{item['timestamp']}] Visual Content: {item['text']}"
                              )

                      output_lines.append("---")
                      
                      result = "\n".join(output_lines)
                      logger.debug(f"Formatted content group: {len(output_lines)} lines")
                      return result

                  except Exception as e:
                      logger.error(f"Error formatting content group: {str(e)}", exc_info=True)
                      raise

              def process_video_content(self, json_data: Dict[str, Any]) -> str:
                  try:
                      summary = json_data.get('video', {}).get('summary', '')
                      visual_texts = []
                      transcripts = []
                      all_timestamps = []
                      speakers = set()

                      for chapter in json_data.get('chapters', []):
                          # Process frames (visual content)
                          if 'frames' in chapter:
                              for frame in chapter['frames']:
                                  if 'text_lines' in frame:
                                      timestamp = frame.get('timestamp_millis', 0) // 1000
                                      all_timestamps.append(timestamp)
                                      for line in frame['text_lines']:
                                          if 'text' in line:
                                              visual_texts.append({
                                                  'timestamp': timestamp,
                                                  'text': line['text'].strip()
                                              })

                          # Process audio segments
                          for segment in chapter.get('audio_segments', []):
                              # Add debug logging
                              logger.debug(f"Processing audio segment: {segment}")
                              
                              if 'text' in segment and 'start_timestamp_millis' in segment:
                                  timestamp = segment['start_timestamp_millis'] // 1000
                                  all_timestamps.append(timestamp)
                                  
                                  speaker_name = segment.get('speaker', {}).get('speaker_name', 'Unknown')
                                  if speaker_name and speaker_name != 'Unknown':
                                      speakers.add(speaker_name)
                                  
                                  transcripts.append({
                                      'timestamp': timestamp,
                                      'text': f"{speaker_name}: {segment['text'].strip()}",
                                      'type': 'Audio'  
                                  })
                                  
                                  logger.debug(f"Added transcript: {timestamp} - {speaker_name}")

                      # Debug logging
                      logger.info(f"Found {len(visual_texts)} visual texts and {len(transcripts)} transcript segments")

                      sections = []
                      if summary:
                          sections.append(self.format_summary_section(summary))

                      # Get categories
                      categories = set()
                      for chapter in json_data.get('chapters', []):
                          for category in chapter.get('iab_categories', []):
                              categories.add(category['category'])

                      # Get time range including both visual and audio timestamps
                      all_timestamps.extend([t['timestamp'] for t in transcripts])
                      time_period = self.get_time_range(all_timestamps)

                      metadata_section = [
                          "# Metadata",
                          f"Time Period: {time_period}",
                          f"Categories: {', '.join(sorted(categories))}",
                          "Content Type: Video",
                          f"Speakers: {', '.join(sorted(speakers)) if speakers else 'None'}"
                      ]
                      sections.append("\n".join(metadata_section))

                      sections.append("# TRANSCRIPT")

                      # Combine and sort all content
                      all_content = visual_texts + transcripts
                      if all_content:
                          # Sort by timestamp
                          all_content.sort(key=lambda x: x['timestamp'])
                          
                          # Group content
                          grouped_content = self.group_content_by_timerange(all_content)
                          
                          for group in grouped_content:
                              formatted_group = self.format_content_group(group, []) 
                              if formatted_group:
                                  sections.append(formatted_group)

                      return "\n\n".join(sections)

                  except Exception as e:
                      logger.error(f"Error processing video content: {str(e)}", exc_info=True)
                      raise

              def process_image_content(self, json_data: Dict[str, Any]) -> str:
                  try:
                      sections = []
                      
                      # Extract and add summary
                      summary = json_data.get('image', {}).get('summary', '')
                      if summary:
                          sections.append(self.format_summary_section(summary))

                      # Extract and add metadata
                      categories = set()
                      # Get categories from image.iab_categories
                      for category in json_data.get('image', {}).get('iab_categories', []):
                          categories.add(category['category'])

                      metadata_section = [
                          "# Metadata",
                          f"Categories: {', '.join(sorted(categories))}",
                          "Content Type: Image"
                      ]
                      sections.append("\n".join(metadata_section))

                      # Extract and add text content
                      sections.append("# TEXT CONTENT")
                      text_content = set()  # Using set to avoid duplicates
                      
                      # Get text from image.text_words
                      for text_line in json_data.get('image', {}).get('text_lines', []):
                          if 'text' in text_line:
                              text_content.add(text_line['text'].strip())

                      if text_content:
                          sections.append("\n".join(sorted(text_content)))
                      else:
                          sections.append("No text content detected")

                      return "\n\n".join(sections)

                  except Exception as e:
                      raise Exception(f"Error processing image content: {str(e)}")

              def process_audio_content(self, json_data: Dict[str, Any]) -> str:
                      try:
                          sections = []
                          
                          # Add summary section
                          summary = json_data.get('audio', {}).get('summary', '')
                          if summary:
                              sections.append(f"# Audio Summary\n{summary}")
              
                          # Get all unique speakers from audio segments
                          audio_segments = json_data.get('audio', {}).get('audio_segments', [])
                          speakers = set()
                          for segment in audio_segments:
                              speaker = segment.get('speaker', {})
                              speaker_name = speaker.get('speaker_name')
                              if speaker_name and speaker_name != 'None':
                                  speakers.add(speaker_name)
                          
                          # Format speakers string
                          speakers_str = ', '.join(sorted(speakers)) if speakers else 'None'
              
                          # Add metadata section
                          metadata = json_data.get('metadata', {})
                          duration_seconds = metadata.get('duration_millis', 0) // 1000  
                          metadata_section = [
                              "# Metadata",
                              f"Time Period: 0-{duration_seconds}s",
                              "Categories: Audio Content",
                              f"Speakers: {speakers_str}"
                          ]
                          sections.append("\n".join(metadata_section))
              
                          # Add transcript section
                          sections.append("# TRANSCRIPT")
                          
                          # Process audio segments
                          transcript_lines = []
                          
                          for segment in audio_segments:
                              # Convert timestamps to seconds using integer division
                              start_time = segment.get('start_timestamp_millis', 0) // 1000
                              end_time = segment.get('end_timestamp_millis', 0) // 1000
                              text = segment.get('text', '')
                              
                              # Safely get speaker information
                              speaker = segment.get('speaker', {})
                              speaker_name = speaker.get('speaker_name', 'Unknown')
                              speaker_label = speaker.get('speaker_label', 'Unknown')
                              
                              # Use speaker_label if speaker_name is 'None' or not present
                              speaker_info = speaker_name if speaker_name and speaker_name != 'None' else speaker_label
                              
                              # Format the transcript line matching the expected format with whole seconds
                              transcript_lines.append(f"\n[{start_time}s] to [{end_time}s]")
                              transcript_lines.append(f"[{start_time}s] Audio Content: {speaker_info}: {text}")
                              transcript_lines.append("---")
              
                          if transcript_lines:
                              sections.append("\n".join(transcript_lines))
                          else:
                              sections.append("No transcript available")
              
                          return "\n\n".join(sections)
              
                      except Exception as e:
                          raise Exception(f"Error processing audio content: {str(e)}")
              
              def process_document_content(self, json_data: Dict[str, Any]) -> str:
                  try:
                      sections = []
                      elements = json_data.get('elements', [])

                      # Document Summary section
                      doc_info = json_data.get('document', {})
                      if doc_info.get('description') or doc_info.get('summary'):
                          sections.append("# DOCUMENT SUMMARY")
                          if doc_info.get('description'):
                              sections.append(doc_info['description'])
                          if doc_info.get('summary'):
                              sections.append(doc_info['summary'])

                      # Metadata section
                      metadata = json_data.get('metadata', {})
                      if metadata:
                          metadata_lines = ["# METADATA"]
                          if 'number_of_pages' in metadata:
                              metadata_lines.append(f"Pages: {metadata['number_of_pages']}")
                          if doc_info.get('statistics'):
                              stats = doc_info['statistics']
                              metadata_lines.append("Statistics:")
                              if 'element_count' in stats:
                                  metadata_lines.append(f"- Elements: {stats['element_count']}")
                              if 'table_count' in stats:
                                  metadata_lines.append(f"- Tables: {stats['table_count']}")
                              if 'figure_count' in stats:
                                  metadata_lines.append(f"- Figures: {stats['figure_count']}")
                              if 'word_count' in stats:
                                  metadata_lines.append(f"- Words: {stats['word_count']}")
                          sections.append("\n".join(metadata_lines))

                      # CONTENT section
                      content_lines = ["# CONTENT"]
                      text_elements = [elem for elem in elements if elem.get('type') == 'TEXT']
                      for elem in text_elements:
                          text_content = elem.get('representation', {}).get('text', '')
                          if text_content:
                              locations = elem.get('locations', [])
                              if locations:
                                  page_indices = [loc.get('page_index', 0) for loc in locations]
                                  page_info = f"[TEXT - Page {', '.join(map(str, page_indices))}]"
                                  sub_type = elem.get('sub_type')
                                  if sub_type:
                                      content_lines.append(f"{page_info} [{sub_type}] {text_content}")
                                  else:
                                      content_lines.append(f"{page_info} {text_content}")
                      sections.append("\n".join(content_lines))
                      
                      # TABLES section
                      table_elements = [elem for elem in elements if elem.get('type') == 'TABLE']
                      if table_elements:
                          table_lines = ["# TABLES"]
                          for table in table_elements:
                              locations = table.get('locations', [])
                              if locations:
                                  page_indices = [loc.get('page_index', 0) for loc in locations]
                                  table_content = table.get('representation', {}).get('text', '')
                                  if table_content:
                                      page_info = f"[TABLE - Page {', '.join(map(str, page_indices))}]"
                                      sub_type = table.get('sub_type')
                                      if sub_type:
                                          table_lines.append(f"{page_info} [{sub_type}]")
                                      else:
                                          table_lines.append(page_info)
                                      table_lines.append(table_content)
                                      table_lines.append("---")
                          sections.append("\n".join(table_lines))
                      
                      # FIGURES section
                      figure_elements = [elem for elem in elements if elem.get('type') == 'FIGURE']
                      if figure_elements:
                          figure_lines = ["# FIGURES"]
                          for figure in figure_elements:
                              figure_section = []
                              locations = figure.get('locations', [])
                              if locations:
                                  page_indices = [loc.get('page_index', 0) for loc in locations]
                                  page_info = f"[FIGURE - Page {', '.join(map(str, page_indices))}]"
                                  sub_type = figure.get('sub_type')
                                  if sub_type:
                                      figure_section.append(f"{page_info} [{sub_type}]")
                                  else:
                                      figure_section.append(page_info)
                              
                              if figure.get('title'):
                                  figure_section.append(f"Title: {figure['title']}")
                              if figure.get('representation', {}).get('text'):
                                  figure_section.append(f"Text: {figure['representation']['text']}")
                              if figure.get('summary'):
                                  figure_section.append(f"Summary: {figure['summary']}")
                              figure_lines.append("\n".join(figure_section))
                              figure_lines.append("---")
                          sections.append("\n".join(figure_lines))
                      
                      return "\n\n".join(sections)
                      
                  except Exception as e:
                      raise Exception(f"Error processing document content: {str(e)}")

              def process_content(self, json_data: Dict[str, Any]) -> str:
                  try:
                      # Get semantic_modality from metadata
                      semantic_modality = json_data.get('metadata', {}).get('semantic_modality', '').upper()
                      
                      if semantic_modality == 'VIDEO':
                          return self.process_video_content(json_data)
                      elif semantic_modality == 'IMAGE':
                          return self.process_image_content(json_data)
                      elif semantic_modality == 'AUDIO':
                          return self.process_audio_content(json_data)
                      elif semantic_modality == 'DOCUMENT':
                          return self.process_document_content(json_data)
                      else:
                          raise Exception(f"Unsupported or unknown content type: {semantic_modality}")

                  except Exception as e:
                      raise Exception(f"Error processing content: {str(e)}")

          def lambda_handler(event, context):
              logger.info(f"Received BDA Invocation response: {event}")
              if event['detail-type'] == 'Bedrock Data Automation Job Succeeded':
                  try:
                      # Extract S3 details from the event
                      detail = event['detail']
                      output_bucket = os.environ['ORGANIZED_BUCKET']
                      base_path = detail['output_s3_location']['name']
                      
                      # Get original filename with underscore and extension without dot
                      original_filename = f"{os.path.splitext(detail['input_s3_object']['name'])[0]}_{os.path.splitext(detail['input_s3_object']['name'])[1][1:]}"
                      # Construct the full S3 path for result.json
                      result_key = f"{base_path}/standard_output/0/result.json"
                      
                      # Create S3 client
                      s3_client = boto3.client('s3')
                      
                      # Download the result.json file
                      response = s3_client.get_object(
                          Bucket=output_bucket,
                          Key=result_key
                      )
                      logger.info(f"Downloaded BDA invocation result")
                      # Read the content of the file
                      result_content = json.loads(response['Body'].read().decode('utf-8'))

                      processor = ContentProcessor()
                      processed_result = processor.process_content(result_content)

                      # Save processed result to Documents folder
                      processed_key = f"Documents/{original_filename}.txt"
                      
                      s3_client.put_object(
                          Bucket=output_bucket,
                          Key=processed_key,
                          Body=processed_result.encode('utf-8'),
                          ContentType='text/plain'
                      )
                      
                      logger.info(f"Saved processed result to s3://{output_bucket}/{processed_key}")
                      
                      return {
                          'statusCode': 200,
                          'body': json.dumps({
                              'message': 'Successfully processed result.json',
                              'result': result_content
                          })
                      }
                      
                  except Exception as e:
                      print(f"Error processing result.json: {str(e)}")
                      return {
                          'statusCode': 500,
                          'body': json.dumps({
                              'message': 'Error processing result.json',
                              'error': str(e)
                          })
                      }
      Runtime: python3.12
      Timeout: 900
      MemorySize: 256
      Environment:
        Variables:
          ORGANIZED_BUCKET: !Ref OrganizedBucket
          CHUNK_SIZE_MS: 60000

  BDAProcessingFunctionRole:
    Type: AWS::IAM::Role
    Condition: IsBedrockDataAutomation
    Properties:
      RoleName: !Sub BDAProcessingFunctionRole-${AWS::StackName}-${ResourceSuffix}
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: !Sub S3Access-${AWS::StackName}-${ResourceSuffix}
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:PutObject
                  - s3:GetObject
                  - s3:ListBucket
                Resource:
                  - !Sub ${OrganizedBucket.Arn}/*
                  - !Sub ${OrganizedBucket.Arn}

  BDAProcessingFunctionLambdaPermission:
    Type: AWS::Lambda::Permission
    Condition: IsBedrockDataAutomation
    Properties:
      FunctionName: !Ref BDAProcessingFunction
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt BDAEventRule.Arn  

# Common Processing Resources
  FileProcessingRule:
    Type: AWS::Events::Rule
    Properties:
      Name: !Sub ${AWS::AccountId}-file-processing-${AWS::StackName}-${ResourceSuffix}
      Description: Rule to process media and non-media files
      EventPattern:
        source:
          - aws.s3
        detail-type:
          - Object Created
        detail:
          bucket:
            name:
              - !Ref MediaBucket
      State: ENABLED
      Targets:
        - Arn: !GetAtt InitialProcessingFunction.Arn
          Id: FileProcessingTarget

  InitialProcessingFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub ${AWS::AccountId}-initial-processing-${AWS::StackName}-${ResourceSuffix}
      Layers:
        - !Ref DependencyLayer
      Handler: index.lambda_handler
      Role: !GetAtt InitialProcessingRole.Arn
      Runtime: python3.12
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          from datetime import datetime

          s3 = boto3.client('s3')
          transcribe = boto3.client('transcribe')
          bedrock_data_automation = boto3.client('bedrock-data-automation-runtime')
          MEDIA_EXTENSIONS = ['.mp3', '.mp4', '.wav', '.flac', '.ogg', '.amr', '.webm']

          def lambda_handler(event, context):
              print('Received event:', json.dumps(event, indent=2))
              account_id = context.invoked_function_arn.split(':')[4]
              region = os.environ['AWS_REGION']
              source_bucket = event['detail']['bucket']['name']
              key = event['detail']['object']['key']
              target_bucket = os.environ['ORGANIZED_BUCKET']
              is_bedrock_data_automation = os.environ.get('IS_BEDROCK_DATA_AUTOMATION', 'false').lower() == 'true'
              
              file_extension = os.path.splitext(key)[1].lower()
              file_name = os.path.splitext(os.path.basename(key))[0]
              file_name_with_extension = f"{file_name}_{file_extension[1:]}"

              if is_bedrock_data_automation:
                  # Processing loging for Bedrock Data Automation 
                  print("Processing with Bedrock Data Automation")
                  try:
                      response = bedrock_data_automation.invoke_data_automation_async(
                          inputConfiguration={
                              's3Uri': f's3://{source_bucket}/{key}'
                          },
                          outputConfiguration={
                              's3Uri': f's3://{target_bucket}/bda-output/{file_name_with_extension}/'
                          },
                          dataAutomationConfiguration={
                              'dataAutomationProjectArn': os.environ['BDA_AUTOMATION_ARN'],
                              'stage': 'LIVE'
                          },
                          notificationConfiguration={
                              'eventBridgeConfiguration': {
                                  'eventBridgeEnabled': True
                              }
                          },
                          dataAutomationProfileArn = f'arn:aws:bedrock:{region}:{account_id}:data-automation-profile/us.data-automation-v1'
                      )
                      
                      print(f"BDA processing started: {response}")
                      return {
                          'statusCode': 200,
                          'body': json.dumps({
                              'message': 'BDA processing started',
                              'response': response
                          })
                      }
                  except Exception as e:
                      print(f"Error processing with BDA: {str(e)}")
                      raise
              else:
                  # Processing logic for Transcribe + Bedrock Default or Transcript + Bedrock Data Automation
                  if file_extension in MEDIA_EXTENSIONS:
                      print(f"Media file detected: {key}")
                      transcription_job_name = f"{file_name_with_extension}-{int(datetime.now().timestamp())}"
                      media_file_uri = f"s3://{source_bucket}/{key}"
                      output_key = f"raw-transcripts/{file_name_with_extension}.json"
                      transcription_params = {
                          'TranscriptionJobName': transcription_job_name,
                          'LanguageCode': 'en-US',
                          'MediaFormat': file_extension[1:],
                          'Media': {'MediaFileUri': media_file_uri},
                          'OutputBucketName': target_bucket,
                          'OutputKey': output_key
                      }
                      try:
                          transcribe.start_transcription_job(**transcription_params)
                          print(f"Transcription job started: {transcription_job_name}")
                          return {
                              'statusCode': 200,
                              'body': json.dumps({
                                  'message': 'Transcription job started',
                                  'jobName': transcription_job_name
                              })
                          }
                      except Exception as e:
                          print(f"Error starting transcription job: {str(e)}")
                          raise
                  else:
                      print(f"Non-media file detected: {key}")
                      new_key = f"Documents/{file_name_with_extension}{file_extension}"
                      try:
                          # Copy the file
                          s3.copy_object(
                              Bucket=target_bucket,
                              CopySource=f"/{source_bucket}/{key}",
                              Key=new_key
                          )
                          print(f"File moved to: {new_key}")
                          return {
                              'statusCode': 200,
                              'body': json.dumps('File processed successfully')
                          }
                      except Exception as e:
                          print(f"Error processing file: {str(e)}")
                          raise
      Environment:
        Variables:
          ORGANIZED_BUCKET: !Ref OrganizedBucket
          IS_BEDROCK_DATA_AUTOMATION: !If [IsBedrockDataAutomation, "true", "false"]
          BDA_AUTOMATION_ARN: !If [IsBedrockDataAutomation, !GetAtt BDAProjectCreator.ProjectArn, "None"]
      Timeout: 900

  InitialProcessingRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub ${AWS::AccountId}-initial-processing-role-${AWS::StackName}-${ResourceSuffix}
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: !Sub S3Access-${AWS::StackName}-${ResourceSuffix}
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:DeleteObject
                Resource:
                  - !Sub ${MediaBucket.Arn}/*
              - Effect: Allow
                Action:
                  - s3:PutObject
                Resource:
                  - !Sub ${OrganizedBucket.Arn}/*
        - PolicyName: !Sub TranscribeAccess-${AWS::StackName}-${ResourceSuffix}
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - transcribe:StartTranscriptionJob
                Resource: '*'
        - !If 
          - IsBedrockDataAutomation
          - PolicyName: !Sub BDA-InvokeAccess-${AWS::StackName}-${ResourceSuffix}
            PolicyDocument:
              Version: '2012-10-17'
              Statement:
                - Effect: Allow
                  Action:
                    - bedrock:InvokeDataAutomationAsync
                  Resource: 
                    - !GetAtt BDAProjectCreator.ProjectArn
                    - !Sub 'arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:data-automation-profile/us.data-automation-v1'
          - !Ref AWS::NoValue

  InitialProcessingPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref InitialProcessingFunction
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt FileProcessingRule.Arn

  # Amazon OpenSearch Resources
  OpenSearchEncryptionPolicy:
    Type: AWS::OpenSearchServerless::SecurityPolicy
    Properties:
      Name: !Sub 'kb-encryption-policy-${AWS::StackName}-${ResourceSuffix}'
      Type: 'encryption'
      Description: 'Encryption policy for Knowledge Base collection'
      Policy: !Sub |
        {
          "Rules":[
            {
              "ResourceType":"collection",
              "Resource":["collection/kb-collection-${AWS::StackName}-${ResourceSuffix}"]
            }
          ],
          "AWSOwnedKey":true
        }

  OpenSearchCollection:
    Type: AWS::OpenSearchServerless::Collection
    DependsOn: OpenSearchEncryptionPolicy
    Properties:
      Name: !Sub 'kb-collection-${AWS::StackName}-${ResourceSuffix}'
      Description: 'Collection for Amazon Bedrock Knowledge Base'
      Type: 'VECTORSEARCH'
      StandbyReplicas: 'DISABLED'

  OpenSearchAccessPolicy:
    Type: AWS::OpenSearchServerless::AccessPolicy
    DependsOn: OpenSearchCollection
    Properties:
      Name: !Sub 'kb-access-policy-${AWS::StackName}-${ResourceSuffix}'
      Type: 'data'
      Description: 'Access policy for Knowledge Base collection'
      Policy: !Sub |
        [{
          "Rules":[
            {
              "ResourceType":"collection",
              "Resource":["collection/kb-collection-${AWS::StackName}-${ResourceSuffix}"],
              "Permission": [
                "aoss:*"
              ]
            },
            {
              "ResourceType":"index",
              "Resource":["index/kb-collection-${AWS::StackName}-${ResourceSuffix}/*"],
              "Permission": [
                "aoss:*"
              ]
            }
          ],
          "Principal":[
            "arn:aws:iam::${AWS::AccountId}:role/${BedrockKnowledgeBaseRole}",
            "arn:aws:iam::${AWS::AccountId}:role/${OpenSearchIndexFunctionRole}"
          ]
        }]

  OpenSearchNetworkPolicy:
    Type: AWS::OpenSearchServerless::SecurityPolicy
    DependsOn: OpenSearchCollection
    Properties:
      Name: !Sub 'kb-network-policy-${AWS::StackName}-${ResourceSuffix}'
      Type: 'network'
      Description: 'Network policy for Knowledge Base collection'
      Policy: !Sub |
        [{
          "Rules":[
            {
                "ResourceType":"collection",
                "Resource":["collection/kb-collection-${AWS::StackName}-${ResourceSuffix}"]
            }, 
            {
                "ResourceType":"dashboard",
                "Resource":["collection/kb-collection-${AWS::StackName}-${ResourceSuffix}"]
            }
          ],
          "AllowFromPublic":true
        }]

  OpenSearchIndexFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: OpenSearchAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'aoss:CreateIndex'
                  - 'aoss:DeleteIndex'
                  - 'aoss:UpdateIndex'
                  - 'aoss:DescribeIndex'
                  - 'aoss:ListIndices'
                  - 'aoss:BatchGetIndex'
                  - 'aoss:SearchIndex'
                  - 'aoss:BatchGetDocument'
                  - 'aoss:CreateDocument'
                  - 'aoss:DeleteDocument'
                  - 'aoss:UpdateDocument'
                  - 'aoss:APIAccessAll' 
                Resource: 
                  - !GetAtt OpenSearchCollection.Arn
                  - !Sub 'arn:aws:aoss:${AWS::Region}:${AWS::AccountId}:collection/kb-collection-${AWS::StackName}-${ResourceSuffix}'

  OpenSearchIndexFunction:
    Type: AWS::Lambda::Function
    Properties:
      Runtime: python3.12
      MemorySize: 512
      Timeout: 900
      Handler: index.handler
      Role: !GetAtt OpenSearchIndexFunctionRole.Arn
      Layers:
        - !Ref DependencyLayer
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import time
          from opensearchpy import OpenSearch, RequestsHttpConnection
          from requests_aws4auth import AWS4Auth
          import re

          def get_aws_auth(region):
              credentials = boto3.Session().get_credentials()
              return AWS4Auth(
                  credentials.access_key,
                  credentials.secret_key,
                  region,
                  'aoss',
                  session_token=credentials.token
              )

          def clean_endpoint(endpoint):
              # Remove 'https://' if present
              if endpoint.startswith('https://'):
                  endpoint = endpoint[8:]
              # Remove any square brackets
              endpoint = endpoint.replace('[', '').replace(']', '')
              return endpoint

          def create_index(host, index_name, region, client):
              
              index_body = {
                  "settings": {
                      "index": {
                          "knn": True,
                          "knn.algo_param.ef_search": 512
                      }
                  },
                  "mappings": {
                      "properties": {
                          "transcripts-field": {
                              "type": "knn_vector",
                              "dimension": 1024,
                              "method": {
                                  "engine": "faiss",
                                  "name": "hnsw",
                                  "space_type": "l2"
                              }
                          },
                          "docs-field": {
                              "type": "knn_vector",
                              "dimension": 1024,
                              "method": {
                                  "engine": "faiss",
                                  "name": "hnsw",
                                  "space_type": "l2"
                              }
                          },
                          "transcripts-chunk": {
                              "type": "text"
                          },
                          "docs-chunk": {
                              "type": "text"
                          },
                          "transcripts-metadata": {
                              "type": "text",
                              "index": False
                          },
                          "docs-metadata": {
                              "type": "text",
                              "index": False
                          }
                      }
                  }
              }

              try:
                  print(f"Creating index {index_name} on host")
                  response = client.indices.create(index=index_name, body=index_body)
                  print(f"Index {index_name} created successfully")
                  time.sleep(30)  # Wait for index to be fully created
                  return True
              except Exception as e:
                  print(f"Error creating index {index_name}: {str(e)}")
                  return False

          def handler(event, context):
              try:
                  if event['RequestType'] in ['Create', 'Update']:
                      collection_endpoint = event['ResourceProperties']['CollectionEndpoint']
                      region = event['ResourceProperties']['Region']
                      access_policy = event['ResourceProperties']['AccessPolicy']
                      
                      print(f"Collection endpoint: {collection_endpoint}")
                      
                      # Wait for access policy to be ready
                      time.sleep(30)

                      #Create OS client
                      clean_host = clean_endpoint(collection_endpoint)
                      
                      client = OpenSearch(
                          hosts=[{'host': clean_host, 'port': 443}],
                          http_auth=get_aws_auth(region),
                          use_ssl=True,
                          verify_certs=True,
                          connection_class=RequestsHttpConnection,
                          timeout=300
                      )
                      # Create indices with retry logic
                      success_docs = False
                      max_retries = 3
                      retry_delay = 10

                      retries = 0
                      while retries < max_retries and not success_docs:
                          try:
                              success_docs = create_index(collection_endpoint, 'docs-index', region, client)
                              if not success_docs:
                                  print("Failed to create docs-index")
                          except Exception as e:
                              print(f"Attempt {retries + 1} for docs-index failed: {str(e)}")
                              retries += 1
                              if retries < max_retries:
                                  time.sleep(retry_delay)

                      if success_docs:
                          cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                      else:
                          cfnresponse.send(event, context, cfnresponse.FAILED, {})
                  else:
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
              except Exception as e:
                  print(f"Error: {str(e)}")
                  cfnresponse.send(event, context, cfnresponse.FAILED, {})

  CreateOpenSearchIndexes:
    Type: Custom::OpenSearchIndexes
    Properties:
      ServiceToken: !GetAtt OpenSearchIndexFunction.Arn
      CollectionEndpoint: !GetAtt OpenSearchCollection.CollectionEndpoint
      Region: !Ref 'AWS::Region'
      AccessPolicy: !Ref OpenSearchAccessPolicy  

  # Amazon Bedrock Knowledgebase Resources
  BedrockKnowledgeBaseRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: bedrock.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: !Sub 'BedrockKBAccess-${AWS::StackName}-${ResourceSuffix}'
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'aoss:CreateCollection'
                  - 'aoss:DeleteCollection'
                  - 'aoss:UpdateCollection'
                  - 'aoss:BatchGetCollection'
                  - 'aoss:CreateAccessPolicy'
                  - 'aoss:CreateSecurityPolicy'
                  - 'aoss:GetAccessPolicy'
                  - 'aoss:GetSecurityPolicy'
                  - 'aoss:ListAccessPolicies'
                  - 'aoss:ListSecurityPolicies'
                  - 'aoss:UpdateAccessPolicy'
                  - 'aoss:UpdateSecurityPolicy'
                  - 'aoss:APIAccessAll'
                Resource: !GetAtt OpenSearchCollection.Arn
              - Effect: Allow
                Action:
                  - 's3:GetObject'
                  - 's3:ListBucket'
                  - 's3:PutObject'
                Resource:
                  - !Sub 'arn:aws:s3:::${OrganizedBucket}'
                  - !Sub 'arn:aws:s3:::${OrganizedBucket}/*'
              - Effect: Allow
                Action:
                  - 's3:*'
                Resource:
                  - !Sub 'arn:aws:s3:::${MultimodalBucket}'
                  - !Sub 'arn:aws:s3:::${MultimodalBucket}/*'
              - Effect: Allow
                Action:
                  - "bedrock:ListFoundationModels"
                  - "bedrock:GetFoundationModel"
                  - "bedrock:InvokeModel"
                Resource: 
                  - !Sub "arn:aws:bedrock:${AWS::Region}::foundation-model/*"
                  - !Sub "arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:custom-model/*"
                  - !Sub "arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:provisioned-model/*"

  BedrockDocsKnowledgeBase:
    Type: AWS::Bedrock::KnowledgeBase
    DependsOn: CreateOpenSearchIndexes
    Properties:
      Name: !Sub '${AWS::AccountId}-documents-kb-${AWS::StackName}-${ResourceSuffix}'
      Description: 'Knowledge base for documents'
      KnowledgeBaseConfiguration:
        Type: 'VECTOR'
        VectorKnowledgeBaseConfiguration:
          EmbeddingModelArn: !Sub 'arn:aws:bedrock:${AWS::Region}::foundation-model/${EmbeddingModelId}'
          SupplementalDataStorageConfiguration:
            SupplementalDataStorageLocations:
                - SupplementalDataStorageLocationType: S3
                  S3Location: 
                    URI: !Sub 's3://${MultimodalBucket}'
      StorageConfiguration:
        Type: 'OPENSEARCH_SERVERLESS'
        OpensearchServerlessConfiguration:
          CollectionArn: !GetAtt OpenSearchCollection.Arn
          FieldMapping:
            VectorField: 'docs-field'
            TextField: 'docs-chunk'
            MetadataField: 'docs-metadata'
          VectorIndexName: 'docs-index'
      RoleArn: !GetAtt BedrockKnowledgeBaseRole.Arn
  
  BedrockDocsDataSource:
    Type: AWS::Bedrock::DataSource
    Condition: IsBedrockDefaultOrBDA
    Properties:
      Name: !Sub '${AWS::AccountId}-documents-ds-${AWS::StackName}-${ResourceSuffix}'
      Description: 'Data source for documents'
      KnowledgeBaseId: !GetAtt BedrockDocsKnowledgeBase.KnowledgeBaseId
      DataSourceConfiguration:
        Type: 'S3'
        S3Configuration:
          BucketArn: !Sub 'arn:aws:s3:::${OrganizedBucket}'
          InclusionPrefixes:
            - 'Documents/'
      VectorIngestionConfiguration:
        ChunkingConfiguration:
          ChunkingStrategy: 'HIERARCHICAL'
          HierarchicalChunkingConfiguration:
            LevelConfigurations:
              - MaxTokens: 1000
              - MaxTokens: 300
            OverlapTokens: 60

  BedrockDocsTBDADataSource:
    Type: AWS::Bedrock::DataSource
    Condition: IsTranscribeWithBDA
    Properties:
      Name: !Sub '${AWS::AccountId}-documents-ds-${AWS::StackName}-${ResourceSuffix}'
      Description: 'Data source for documents'
      KnowledgeBaseId: !GetAtt BedrockDocsKnowledgeBase.KnowledgeBaseId
      DataSourceConfiguration:
        Type: 'S3'
        S3Configuration:
          BucketArn: !Sub 'arn:aws:s3:::${OrganizedBucket}'
          InclusionPrefixes:
            - 'Documents/'
      VectorIngestionConfiguration:
        ChunkingConfiguration:
          ChunkingStrategy: 'HIERARCHICAL'
          HierarchicalChunkingConfiguration:
            LevelConfigurations:
              - MaxTokens: 1000
              - MaxTokens: 300
            OverlapTokens: 60
        ParsingConfiguration:
          ParsingStrategy: BEDROCK_DATA_AUTOMATION
          BedrockDataAutomationConfiguration: 
            ParsingModality: MULTIMODAL
  
  RetrievalFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${AWS::AccountId}-chatbot-retrieval-${AWS::StackName}-${ResourceSuffix}'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      Policies:
        - PolicyName: BedrockAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'bedrock:InvokeModel'
                  - 'bedrock:Retrieve'
                  - 'bedrock:RetrieveAndGenerate'
                  - 'bedrock-agent-runtime:Retrieve'
                  - 'bedrock-agent-runtime:RetrieveAndGenerate'
                  - 'bedrock-runtime:InvokeModel'
                  - 'bedrock:ApplyGuardrail'
                Resource: '*'
        - PolicyName: CloudWatchLogsAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'logs:CreateLogGroup'
                  - 'logs:CreateLogStream'
                  - 'logs:PutLogEvents'
                Resource: '*'

  RetrievalFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${AWS::AccountId}-chatbot-retrieval-${AWS::StackName}-${ResourceSuffix}'
      Runtime: python3.12
      Handler: index.lambda_handler
      Role: !GetAtt RetrievalFunctionRole.Arn
      MemorySize: 512
      Timeout: 900
      Environment:
        Variables:
          OPS_KNOWLEDGE_BASE_ID: !Ref BedrockDocsKnowledgeBase
          MODEL_ID: !Ref ModelId
      Code:
        ZipFile: |
          import os
          import re
          import boto3
          import random
          import string

          boto3_session = boto3.session.Session()
          region = boto3_session.region_name

          # create a boto3 bedrock client
          bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime')
          bedrock_client = boto3.client(service_name='bedrock-runtime')

          # get knowledge base id from environment variable
          ops_kb_id = os.environ.get("OPS_KNOWLEDGE_BASE_ID")


          def get_contexts(retrievalResults):
              contexts = []
              video_extensions = {'mp3', 'mp4', 'wav', 'flac', 'ogg', 'amr', 'webm', 'mov'}
              
              for result in retrievalResults:
                  filename = result['location']['s3Location']['uri'].split('/')[-1]
                  # Get actual extension from filename (part after last underscore, before any dot)
                  actual_extension = filename.split('_')[-1].split('.')[0]
                  
                  source = 'video' if actual_extension in video_extensions else 'file'
                  print(actual_extension, source)
                  contexts.extend([
                      f'<source>{source}</source>',
                      f'<location>{filename}</location>',
                      result['content']['text']
                  ])
              return contexts

          def retrieveResults(query, kbId):
              results =  bedrock_agent_runtime_client.retrieve(
                  retrievalQuery= {
                      'text': query
                  },
                  knowledgeBaseId=kbId,
                  retrievalConfiguration= {
                      'vectorSearchConfiguration': {
                          'numberOfResults': 3,
                          'overrideSearchType': "HYBRID",
                      }
                  }
              )
              retrievalResults = results['retrievalResults']
              return get_contexts(retrievalResults)

          def generate_conversation(model_id,
                                  system_prompts,
                                  messages,
                                  guardrail_id=None,
                                  guardrail_version=None,
                                  temperature=None,
                                  top_p=None):
              print("Generating message with model %s", model_id)

              # Validate and set default values
              try:
                  # Temperature validation: must be float between 0 and 1
                  validated_temperature = 0.1  # default
                  if temperature is not None:
                      temp_float = float(temperature)
                      if 0 <= temp_float <= 1:
                          validated_temperature = temp_float

                  # Top P validation: must be float between 0 and 1
                  validated_top_p = 0.9  # default
                  if top_p is not None:
                      top_p_float = float(top_p)
                      if 0 <= top_p_float <= 1:
                          validated_top_p = top_p_float

                  # Guardrail validation: both id and version must be non-empty strings
                  validated_guardrail_id = None
                  validated_guardrail_version = None
                  if guardrail_id and isinstance(guardrail_id, str) and guardrail_id.strip():
                      validated_guardrail_id = guardrail_id.strip()
                      if guardrail_version and isinstance(guardrail_version, str) and guardrail_version.strip():
                          validated_guardrail_version = guardrail_version.strip()

              except (ValueError, TypeError) as e:
                  print(f"Validation error: {str(e)}. Using default values.")
                  validated_temperature = 0.1
                  validated_top_p = 0.9

              # Base inference parameters
              inference_config = {
                  "temperature": validated_temperature,
                  "topP": validated_top_p
              }

              # Prepare request parameters
              request_params = {
                  "modelId": model_id,
                  "messages": messages,
                  "system": system_prompts,
                  "inferenceConfig": inference_config
              }

              # Add guardrail configuration if ID is provided
              if guardrail_id and isinstance(guardrail_id, str) and guardrail_id.strip():
                  guardrail_config = {
                      "guardrailIdentifier": guardrail_id.strip(),
                      "trace": "enabled"  # You can make this configurable if needed
                  }
                  
                  # Add version if provided
                  if guardrail_version and isinstance(guardrail_version, str) and guardrail_version.strip():
                      guardrail_config["guardrailVersion"] = guardrail_version.strip()
                  
                  request_params["guardrailConfig"] = guardrail_config


              # Send the message
              response = bedrock_client.converse(**request_params)

              # Log configuration and token usage
              print("Using configuration:", {
                  "temperature": validated_temperature,
                  "topP": validated_top_p,
                  "guardrailId": validated_guardrail_id,
                  "guardrailVersion": validated_guardrail_version,
                  "modelId": model_id
              })
              
              token_usage = response['usage']
              print("Input tokens: %s", token_usage['inputTokens'])
              print("Output tokens: %s", token_usage['outputTokens'])
              print("Total tokens: %s", token_usage['totalTokens'])
              print("Stop reason: %s", response['stopReason'])

              return response

          def lambda_handler(event, context):
              # Regex pattern for model ID validation
              model_pattern = r'^(arn:aws(-[^:]+)?:bedrock:[a-z0-9-]{1,20}:((:foundation-model/[a-z0-9-]{1,63}[.]{1}[a-z0-9-]{1,63}([.:]?[a-z0-9-]{1,63}))|([0-9]{12}:provisioned-model/[a-z0-9]{12})|([0-9]{12}:imported-model/[a-z0-9]{12})|([0-9]{12}:application-inference-profile/[a-z0-9]{12})|([0-9]{12}:inference-profile/((([a-z-]{2,8}.)[a-z0-9-]{1,63}[.]{1}[a-z0-9-]{1,63}([.:]?[a-z0-9-]{1,63}))))|([0-9]{12}:default-prompt-router/[a-zA-Z0-9-:.]+)))|(([a-z]{2}[.]{1})([a-z0-9-]{1,63}[.]{1}[a-z0-9-]{1,63}([.:]?[a-z0-9-]{1,63})))|([a-z0-9-]{1,63}[.]{1}[a-z0-9-]{1,63}([.:]?[a-z0-9-]{1,63}))|arn:aws(-[^:]+)?:sagemaker:[a-z0-9-]{1,20}:[0-9]{12}:endpoint/[a-z0-9-]{1,63}$'

              # Try to get model_id from event and validate it
              event_model_id = event["modelId"]
              if event_model_id and re.match(model_pattern, event_model_id):
                  model_id = event_model_id
              else:
                  # Fall back to environment variable if event model_id is invalid or missing
                  model_id = os.environ.get("MODEL_ID")

              query = event["question"]
              messages = event["messages"]
              guardrail_id = event.get("guardrailId")
              guardrail_version = event.get("guardrailVersion")
              temperature = event.get("temperature")
              top_p = event.get("topP")

              retrieved_info = retrieveResults(query, ops_kb_id)

              prompt = [{"text": f"""
              You are a question answering agent. The user will provide you with a question. 
              Your job is to answer the user's question using only the following sets of information within <context> </context> tag.
              
              INPUT FORMAT:
              - Each set of information starts with a <source>video</source> or <source>file</source> tag signifying the source
              - Followed by a <location> </location> tag containing a value
              - Information may include timestamps [seconds] and metadata [text] in square brackets
              - Metadata tags examples: [TEXT - Page 0], [PARAGRAPH], [FIGURE - Page X], [LOGO]
              
              RESPONSE FORMAT:
              Part 1 - Answer:
              - MUST be wrapped in <answer> </answer> tags. DO NOT DEVIATE.
              - Should be fluid and natural without unnecessary line breaks
              - YOU MUST remove ALL metadata tags (like [TEXT - Page 0]) from your answer
              - For video sources:
                * You MUST include both location and timestamp in individual square brackets: [timestamp location]
                * Example: [23 file_mp4.txt] The speaker mentioned AWS is currently..
                * Use ordered lists for multiple points
              
              Part 2 - Location:
              - List ONLY the <location> tags from sources that contributed to your answer
              - Format must be exactly:
              <location>file_name.pdf</location>
              <location>another_file_mp4.txt</location>
              - One location tag per line
              - DO NOT include any other tags or text
              
              ERROR HANDLING:
              - If insufficient information is found or you cannot make a conclusion, state that you cannot provide an exact answer and request more context if appropriate. DO NOT add <location> </location> or <answer> </answer> tags
              
              Here are the search results:
              <context>
              {retrieved_info}
              </context>
              """}]

              
              response = generate_conversation(
                  model_id,
                  prompt,
                  messages,
                  guardrail_id=guardrail_id,
                  guardrail_version=guardrail_version,
                  temperature=temperature,
                  top_p=top_p
              )
              generated_text = response['output']['message']
              print('Generated Text: ', generated_text)

              return {
                  'statusCode': 200,
                  'body': {"question": query.strip(), "answer": generated_text}
              }

  # React Application Resources
  ApplicationHostBucket:
    Type: AWS::S3::Bucket
    DeletionPolicy: Delete
    Properties:
      BucketName: !Sub ${AWS::AccountId}-app-host-bucket-${AWS::StackName}-${ResourceSuffix}
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  MediaBucketCloudFrontOAC:
    Type: AWS::CloudFront::OriginAccessControl
    Properties:
      OriginAccessControlConfig:
        Name: !Sub ${AWS::StackName}-media-bucket-oac-${AWS::StackName}-${ResourceSuffix}
        Description: Origin Access Control for Media Bucket
        SigningBehavior: always    
        SigningProtocol: sigv4    
        OriginAccessControlOriginType: s3

  EdgeRequestPolicy:
    Type: AWS::CloudFront::OriginRequestPolicy
    Properties:
      OriginRequestPolicyConfig:
        Name: !Sub "EdgeRequest-${ResourceSuffix}"
        HeadersConfig:
          HeaderBehavior: none
        QueryStringsConfig:
          QueryStringBehavior: whitelist
          QueryStrings:
            - auth
        CookiesConfig:
          CookieBehavior: none
        Comment: Origin request policy to forward auth query string

  CloudFrontDistribution:
    Type: AWS::CloudFront::Distribution
    Properties:
      DistributionConfig:
        Enabled: true
        Comment: !Sub 'Distribution for ${AWS::AccountId} media and application buckets'
        DefaultRootObject: index.html
        Origins:
          - Id: MediaBucketOrigin
            DomainName: !GetAtt MediaBucket.RegionalDomainName
            S3OriginConfig:
              OriginAccessIdentity: ''
            OriginAccessControlId: !GetAtt MediaBucketCloudFrontOAC.Id
          - Id: ApplicationHostBucketOrigin
            DomainName: !GetAtt ApplicationHostBucket.RegionalDomainName
            S3OriginConfig:
              OriginAccessIdentity: ''
            OriginAccessControlId: !GetAtt MediaBucketCloudFrontOAC.Id
        CacheBehaviors:
          - PathPattern: "*.html"
            TargetOriginId: ApplicationHostBucketOrigin
            ViewerProtocolPolicy: redirect-to-https
            AllowedMethods:
              - GET
              - HEAD
              - OPTIONS
            CachePolicyId: 4135ea2d-6df8-44a3-9df3-4b5a84be39ad  # Managed-CachingDisabled
            OriginRequestPolicyId: 88a5eaf4-2fd4-4709-b370-b4c650ea3fcf  # Managed-CORS-S3Origin
            ResponseHeadersPolicyId: 5cc3b908-e619-4b99-88e5-2cf7f45965bd  # Managed-CORS-with-Preflight-and-SecurityHeaders
            Compress: true
          - PathPattern: "*.js"
            TargetOriginId: ApplicationHostBucketOrigin
            ViewerProtocolPolicy: redirect-to-https
            AllowedMethods:
              - GET
              - HEAD
              - OPTIONS
            CachePolicyId: 4135ea2d-6df8-44a3-9df3-4b5a84be39ad  # Managed-CachingDisabled
            OriginRequestPolicyId: 88a5eaf4-2fd4-4709-b370-b4c650ea3fcf  # Managed-CORS-S3Origin
            ResponseHeadersPolicyId: 5cc3b908-e619-4b99-88e5-2cf7f45965bd  # Managed-CORS-with-Preflight-and-SecurityHeaders
            Compress: true
          - PathPattern: "*.css"
            TargetOriginId: ApplicationHostBucketOrigin
            ViewerProtocolPolicy: redirect-to-https
            AllowedMethods:
              - GET
              - HEAD
              - OPTIONS
            CachePolicyId: 4135ea2d-6df8-44a3-9df3-4b5a84be39ad  # Managed-CachingDisabled
            OriginRequestPolicyId: 88a5eaf4-2fd4-4709-b370-b4c650ea3fcf  # Managed-CORS-S3Origin
            ResponseHeadersPolicyId: 5cc3b908-e619-4b99-88e5-2cf7f45965bd  # Managed-CORS-with-Preflight-and-SecurityHeaders
            Compress: true
        DefaultCacheBehavior:
          TargetOriginId: MediaBucketOrigin
          ViewerProtocolPolicy: redirect-to-https
          AllowedMethods:
            - GET
            - HEAD
            - OPTIONS
          CachedMethods:
            - GET
            - HEAD
            - OPTIONS
          CachePolicyId: 4135ea2d-6df8-44a3-9df3-4b5a84be39ad  # Managed-CachingDisabled
          OriginRequestPolicyId: !Ref EdgeRequestPolicy
          ResponseHeadersPolicyId: 60669652-455b-4ae9-85a4-c4c02393f86c  # Managed-SimpleCORS
        PriceClass: PriceClass_100
        HttpVersion: http2
        IPV6Enabled: true

  MediaBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref MediaBucket
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: AllowCloudFrontServicePrincipal
            Effect: Allow
            Principal:
              Service: cloudfront.amazonaws.com
            Action: s3:GetObject
            Resource: !Sub '${MediaBucket.Arn}/*'
            Condition:
              StringEquals:
                'AWS:SourceArn': !Sub 'arn:aws:cloudfront::${AWS::AccountId}:distribution/${CloudFrontDistribution.Id}'

  ApplicationHostBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref ApplicationHostBucket
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Sid: AllowCloudFrontServicePrincipal
            Effect: Allow
            Principal:
              Service: cloudfront.amazonaws.com
            Action: s3:GetObject
            Resource: !Sub '${ApplicationHostBucket.Arn}/*'
            Condition:
              StringEquals:
                'AWS:SourceArn': !Sub 'arn:aws:cloudfront::${AWS::AccountId}:distribution/${CloudFrontDistribution.Id}'

  ChatbotUserPool:
    Type: AWS::Cognito::UserPool
    Properties:
      AccountRecoverySetting:
        RecoveryMechanisms:
          - Name: verified_email
            Priority: 2
      AdminCreateUserConfig:
        AllowAdminCreateUserOnly: false
      EmailVerificationMessage: "The verification code to your new account is {####}"
      EmailVerificationSubject: "Verify your new account"
      UsernameAttributes:
        - email
      AutoVerifiedAttributes:
        - email
      Schema:
        - Mutable: true
          Name: email
          Required: true
      VerificationMessageTemplate:
        DefaultEmailOption: CONFIRM_WITH_CODE
        EmailMessage: "The verification code to your new account is {####}"
        EmailSubject: "Verify your new account"
    UpdateReplacePolicy: Delete
    DeletionPolicy: Delete

  ChatbotUserPoolClient:
    Type: AWS::Cognito::UserPoolClient
    Properties:
      AllowedOAuthFlows:
        - implicit
        - code
      AllowedOAuthFlowsUserPoolClient: true
      AllowedOAuthScopes:
        - profile
        - phone
        - email
        - openid
        - aws.cognito.signin.user.admin
      CallbackURLs:
        - https://example.com
      ExplicitAuthFlows:
        - ALLOW_USER_PASSWORD_AUTH
        - ALLOW_ADMIN_USER_PASSWORD_AUTH
        - ALLOW_USER_SRP_AUTH
        - ALLOW_REFRESH_TOKEN_AUTH
      GenerateSecret: false
      SupportedIdentityProviders:
        - COGNITO
      UserPoolId: !Ref ChatbotUserPool
      AccessTokenValidity: 5
      IdTokenValidity: 5
      RefreshTokenValidity: 7
      TokenValidityUnits:
        AccessToken: minutes
        IdToken: minutes
        RefreshToken: days

  ChatbotIdentityPool:
    Type: AWS::Cognito::IdentityPool
    Properties:
      AllowUnauthenticatedIdentities: false
      CognitoIdentityProviders:
        - ClientId: !Ref ChatbotUserPoolClient
          ProviderName: !GetAtt ChatbotUserPool.ProviderName
          ServerSideTokenCheck: true

  ChatbotIdentityPoolAuthRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Federated: cognito-identity.amazonaws.com
            Action: sts:AssumeRoleWithWebIdentity
            Condition:
              StringEquals:
                'cognito-identity.amazonaws.com:aud': !Ref ChatbotIdentityPool
              'ForAnyValue:StringLike':
                'cognito-identity.amazonaws.com:amr': authenticated
      Policies:
        - PolicyName: CognitoAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - cognito-identity:GetCredentialsForIdentity
                Resource: 
                  - !Sub arn:aws:cognito-identity:${AWS::Region}:${AWS::AccountId}:identitypool/${ChatbotIdentityPool}
        - PolicyName: LambdaInvokePolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - lambda:InvokeFunction
                Resource: !Sub 'arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:${RetrievalFunction}'
        - PolicyName: AdditionalServicesPolicy
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - cognito-identity:GetId
                  - cognito-identity:GetCredentialsForIdentity
                Resource: '*'
              - Effect: Allow
                Action:
                  - s3:GetObject
                  - s3:ListBucket
                  - s3:PutObject
                Resource:
                  - !Sub 'arn:aws:s3:::${MediaBucket}'
                  - !Sub 'arn:aws:s3:::${MediaBucket}/*'
        - PolicyName: BedrockAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - 'bedrock:StartIngestionJob'
                  - 'bedrock:ListIngestionJobs'
                Resource: 
                  - !Sub 'arn:aws:bedrock:${AWS::Region}:${AWS::AccountId}:knowledge-base/${BedrockDocsKnowledgeBase.KnowledgeBaseId}'

  ChatbotIdentityPoolRoleAttachment:
    Type: AWS::Cognito::IdentityPoolRoleAttachment
    Properties:
      IdentityPoolId: !Ref ChatbotIdentityPool
      Roles:
        authenticated: !GetAtt ChatbotIdentityPoolAuthRole.Arn

Outputs:
  ReactAppS3Source:
    Description: S3 bucket to add data, media files
    Value: !Ref MediaBucket
  ReactAppLambdaFunctionName:
    Description: Name of the retrieval Lambda function
    Value: !Ref RetrievalFunction
  ReactAppCloudFrontDomainName:
      Description: CloudFront Domain Name (ID only)
      Value: !Select [0, !Split [".cloudfront.net", !GetAtt CloudFrontDistribution.DomainName]]
  ReactAppUserPoolId:
    Description: Cognito User Pool ID
    Value: !Ref ChatbotUserPool
  ReactAppUserPoolClientId:
    Description: Cognito User Pool Client ID
    Value: !Ref ChatbotUserPoolClient
  ReactAppIdentityPoolId:
    Description: ID of the Cognito Identity Pool
    Value: !Ref ChatbotIdentityPool
  ReactAppDocsKbId:
    Description: Documents Knowledge Base ID
    Value: !Ref BedrockDocsKnowledgeBase
  ReactAppDocumentsDsId:
    Description: Documents Knowledge Base Datasource ID
    Value: !Select 
      - 1
      - !Split
        - "|"
        - !If
          - IsBedrockDefaultOrBDA
          - !Ref BedrockDocsDataSource
          - !If 
            - IsTranscribeWithBDA
            - !Ref BedrockDocsTBDADataSource
            - !Ref "AWS::NoValue"
  ReactAppHostBucket:
    Description: S3 bucket to add chatbot application files
    Value: !Ref ApplicationHostBucket
